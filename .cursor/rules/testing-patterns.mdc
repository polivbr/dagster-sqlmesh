
# Testing Patterns for dg-sqlmesh (dagster-sqlmesh folder)

## Test Project Structure

The test project in [tests/sqlmesh_project/](mdc:tests/sqlmesh_project/) demonstrates the complete integration:

### Configuration

- [tests/sqlmesh_project/config.yaml](mdc:tests/sqlmesh_project/config.yaml) - DuckDB configuration
- [tests/sqlmesh_project/external_models.yaml](mdc:tests/sqlmesh_project/external_models.yaml) - External model definitions

### Models

- `models/stg/` - Staging models (raw to clean)
- `models/marts/` - Mart models (business logic)
- Follows standard SQLMesh patterns with MODEL() declarations

### Data Loading

- [tests/load_jaffle_data.py](mdc:tests/load_jaffle_data.py) - Loads CSV data into DuckDB
- [tests/jaffle-data/](mdc:tests/jaffle-data/) - Source CSV files
- Creates tables matching external model definitions

## Individual Asset Testing

### Asset Creation Testing

```python
# Test individual asset creation
def test_sqlmesh_assets_factory():
    sqlmesh_resource = SQLMeshResource(project_dir="tests/sqlmesh_project")
    assets = sqlmesh_assets_factory(sqlmesh_resource=sqlmesh_resource)
    
    # Verify individual assets are created
    assert len(assets) > 0
    for asset in assets:
        assert hasattr(asset, 'key')
        assert hasattr(asset, 'group_name')
        assert hasattr(asset, 'check_specs')
```

### Shared Execution Testing

```python
# Test shared SQLMesh execution
def test_shared_execution():
    # Mock SQLMeshResultsResource
    sqlmesh_results = SQLMeshResultsResource()
    
    # First asset should trigger SQLMesh execution
    assert not sqlmesh_results.has_results("run_1")
    
    # After execution, results should be stored
    sqlmesh_results.store_results("run_1", {"test": "data"})
    assert sqlmesh_results.has_results("run_1")
    
    # Subsequent assets should use existing results
    results = sqlmesh_results.get_results("run_1")
    assert results["test"] == "data"
```

### Asset Status Testing

```python
# Test asset status determination
def test_asset_status_determination():
    # Test successful materialization
    result = MaterializeResult(
        asset_key=AssetKey(["test", "model"]),
        check_results=[AssetCheckResult(check_name="audit_1", passed=True)]
    )
    assert result.check_results[0].passed is True
    
    # Test audit failure
    result = MaterializeResult(
        asset_key=AssetKey(["test", "model"]),
        check_results=[AssetCheckResult(check_name="audit_1", passed=False)]
    )
    assert result.check_results[0].passed is False
```

## Event Capture Testing

### Console Event Testing

```python
# Test event capture
def test_event_capture():
    console = SQLMeshEventCaptureConsole()
    
    # Test failed models event
    failed_event = LogFailedModels(errors=[NodeExecutionFailedError("test error")])
    console._handle_log_failed_models(failed_event)
    assert len(console.get_failed_models_events()) == 1
    
    # Test skipped models event
    skipped_event = LogSkippedModels(snapshot_names={"test.model"})
    console._handle_log_skipped_models(skipped_event)
    assert len(console.get_skipped_models_events()) == 1
    
    # Test evaluation event
    eval_event = UpdateSnapshotEvaluationProgress(
        snapshot=MockSnapshot(),
        batch_idx=1,
        duration_ms=100,
        num_audits_passed=2,
        num_audits_failed=0
    )
    console._handle_update_snapshot_evaluation(eval_event)
    assert len(console.get_evaluation_events()) == 1
```

### Event Processing Testing

```python
# Test event processing for asset checks
def test_event_processing():
    # Mock failed models events
    failed_events = [
        {
            "model_name": "test.model",
            "error": "Audit failed: value < 0",
            "audit_name": "assert_positive_values"
        }
    ]
    
    # Process events into asset check results
    check_results = process_failed_models_events(failed_events, asset_key)
    assert len(check_results) == 1
    assert check_results[0].passed is False
    assert "assert_positive_values" in check_results[0].metadata["sqlmesh_audit_name"]
```

## Integration Testing

### Complete Integration Testing

```python
# Test complete SQLMesh integration
def test_sqlmesh_integration():
    # Create definitions
    defs = sqlmesh_definitions_factory(
        project_dir="tests/sqlmesh_project",
        gateway="duckdb",
        enable_schedule=False
    )
    
    # Verify assets are created
    assert len(defs.assets) > 0
    
    # Verify resources are configured
    assert "sqlmesh" in defs.resources
    assert "sqlmesh_results" in defs.resources
    
    # Verify job is created
    assert "sqlmesh_job" in defs.jobs
```

### External Asset Testing

```python
# Test external asset handling
def test_external_assets():
    translator = SQLMeshTranslator()
    
    # Test external asset key mapping
    external_key = translator.get_external_asset_key('"main"."external"."customers"')
    assert external_key == AssetKey(["sling", "customers"])
    
    # Test external dependency detection
    is_external = translator.is_external_dependency(context, "external.source")
    assert is_external is True
```

## Error Handling Testing

### Retry Policy Testing

```python
# Test no retry policy
def test_no_retry_policy():
    # Verify assets have no retry tags
    for asset in assets:
        assert asset.tags["dagster/max_retries"] == "0"
        assert asset.tags["dagster/retry_on_asset_or_op_failure"] == "false"
    
    # Verify job has no retry tags
    job = define_asset_job(name="test_job", selection=assets)
    assert job.tags["dagster/max_retries"] == "0"
    assert job.tags["dagster/retry_on_asset_or_op_failure"] == "false"
```

### Exception Handling Testing

```python
# Test SQLMesh-specific exception handling
def test_sqlmesh_exceptions():
    # Test CircuitBreakerError handling
    with pytest.raises(CircuitBreakerError):
        sqlmesh_resource.materialize_assets_threaded(models)
    
    # Test PlanError handling
    with pytest.raises(PlanError):
        sqlmesh_resource.plan()
    
    # Test NodeExecutionFailedError handling
    with pytest.raises(Exception, match="Model.*was skipped"):
        # Simulate skipped model
        pass
```

## Tag Convention Testing

### Tag Parsing Testing

```python
# Test tag convention parsing
def test_tag_convention():
    translator = SQLMeshTranslator()
    
    # Mock model with Dagster tags
    model = MockModel(tags={
        "dagster:group_name:custom_group",
        "dagster:owner:data_team",
        "production",
        "critical"
    })
    
    # Test property extraction
    group_name = translator._get_dagster_property_from_tags(model, "group_name")
    assert group_name == "custom_group"
    
    owner = translator._get_dagster_property_from_tags(model, "owner")
    assert owner == "data_team"
    
    # Test tag filtering
    tags = translator.get_tags(context, model)
    assert "production" in tags
    assert "critical" in tags
    assert "dagster:group_name:custom_group" not in tags  # Filtered out
```

## Version Mapping Testing

### Code Version Testing

```python
# Test code version mapping
def test_code_version_mapping():
    # Mock model with data_hash
    model = MockModel(data_hash="abc123")
    
    # Extract code version
    code_version = str(getattr(model, "data_hash", "")) if hasattr(model, "data_hash") and getattr(model, "data_hash") else None
    assert code_version == "abc123"
```

### Data Version Testing

```python
# Test data version mapping
def test_data_version_mapping():
    # Mock snapshot with version
    snapshot = MockSnapshot(version="v1.2.3")
    
    # Extract data version
    snapshot_version = getattr(snapshot, "version", None)
    data_version = DataVersion(str(snapshot_version)) if snapshot_version else None
    assert data_version == DataVersion("v1.2.3")
```

## Performance Testing

### Execution Performance Testing

```python
# Test execution performance
def test_execution_performance():
    # Measure single SQLMesh execution time
    start_time = time.time()
    sqlmesh_resource.materialize_assets_threaded(models)
    execution_time = time.time() - start_time
    
    # Verify reasonable execution time
    assert execution_time < 30  # Should complete within 30 seconds
    
    # Verify shared execution doesn't duplicate work
    start_time = time.time()
    # Second execution should use cached results
    execution_time = time.time() - start_time
    assert execution_time < 1  # Should be very fast with cached results
```

## Test Data Management

### Test Data Loading

```python
# Test data loading
def test_data_loading():
    # Load test data
    load_jaffle_data()
    
    # Verify data is loaded
    with duckdb.connect("tests/sqlmesh_project/duck.db") as conn:
        result = conn.execute("SELECT COUNT(*) FROM raw_source_customers").fetchone()
        assert result[0] == 2583
        
        result = conn.execute("SELECT COUNT(*) FROM raw_source_orders").fetchone()
        assert result[0] == 657460
```

### Test Data Cleanup

```python
# Test data cleanup
def test_data_cleanup():
    # Clean up test data
    cleanup_test_data()
    
    # Verify cleanup
    with duckdb.connect("tests/sqlmesh_project/duck.db") as conn:
        result = conn.execute("SELECT COUNT(*) FROM raw_source_customers").fetchone()
        assert result[0] == 0
```

---


---
